\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{xcolor}

% Page geometry
\geometry{
    margin=2.5cm,
    top=3cm,
    bottom=3cm
}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=red
}

% Custom commands
\newcommand{\MGMT}{\textit{MGMT}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\argmin}{\operatorname{argmin}}

\title{Methods: Genome-Wide Methylation Analysis for \MGMT{} Methylation Status Prediction using Machine Learning}

\author{
    Author Name$^{1,*}$ \\
    \small $^1$Department of Bioinformatics, Institution Name \\
    \small $^*$Corresponding author: email@institution.edu
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We present a comprehensive computational pipeline for predicting \MGMT{} (\textit{O6}-methylguanine-DNA methyltransferase) promoter methylation status from genome-wide DNA methylation data obtained through Oxford Nanopore Technologies sequencing. The pipeline employs advanced feature engineering, multiple machine learning algorithms, and ensemble methods to achieve robust prediction performance. Our approach extends beyond traditional \MGMT{} promoter-focused analysis by incorporating genome-wide methylation patterns, extracting 98 comprehensive features, and utilizing seven different machine learning algorithms. The modular implementation includes data preprocessing, feature extraction, model training, evaluation, and prediction components, all orchestrated through a scalable Nextflow workflow. This methodology provides a foundation for precision medicine applications in glioblastoma treatment selection.
\end{abstract}

\section{Introduction}

\MGMT{} (\textit{O6}-methylguanine-DNA methyltransferase) promoter methylation serves as a critical biomarker for predicting treatment response to alkylating agents in glioblastoma patients \citep{hegi2005mgmt, stupp2009effects}. Traditional analysis focuses exclusively on the canonical \MGMT{} promoter region (chr10:129,466,683--129,467,448, GRCh38), but emerging evidence suggests that genome-wide methylation patterns may provide additional predictive information \citep{bady2012mgmt, capper2018dna}.

Long-read sequencing technologies, particularly Oxford Nanopore Technologies (ONT), enable direct detection of DNA methylation modifications across the genome without bisulfite conversion \citep{rand2017mapping}. The modkit package provides standardized bedmethyl output format for methylation analysis \citep{ont2023modkit}. Here, we describe a comprehensive computational framework that leverages genome-wide methylation data to predict \MGMT{} methylation status using advanced machine learning approaches.

\section{Materials and Methods}

\subsection{Data Input and Preprocessing}

\subsubsection{Bedmethyl File Format}

The pipeline processes bedmethyl files generated by Oxford Nanopore's modkit package. Each record $r_i$ contains the following information:

\begin{equation}
r_i = (c_i, s_i, e_i, t_i, \sigma_i, st_i, ts_i, te_i, rgb_i, cov_i, m_i, n_{mod,i}, n_{can,i}, n_{other,i}, n_{del,i}, n_{fail,i}, n_{diff,i})
\end{equation}

where $c_i$ represents chromosome, $s_i$ and $e_i$ are start and end positions, $t_i$ is modification type, $cov_i$ is coverage, $m_i$ is methylation percentage (0--100\%), and $n_{mod,i}$, $n_{can,i}$ represent modified and canonical call counts, respectively.

\subsubsection{Quality Filtering}

Methylation sites are filtered based on coverage threshold to ensure data quality:

\begin{equation}
S_{filtered} = \{s \in S \mid cov(s) \geq \theta_{cov}\}
\end{equation}

where $S$ represents the set of all methylation sites and $\theta_{cov}$ is the minimum coverage threshold (default: 3).

\subsubsection{Data Validation}

For each sample $j$, we compute comprehensive quality metrics:

\textbf{Coverage Statistics:}
\begin{align}
\mu_{cov,j} &= \frac{1}{n_j} \sum_{i=1}^{n_j} cov_{i,j} \\
\sigma_{cov,j} &= \sqrt{\frac{1}{n_j} \sum_{i=1}^{n_j} (cov_{i,j} - \mu_{cov,j})^2}
\end{align}

\textbf{Methylation Statistics:}
\begin{align}
\mu_{meth,j} &= \frac{1}{n_j} \sum_{i=1}^{n_j} m_{i,j} \\
\sigma_{meth,j} &= \sqrt{\frac{1}{n_j} \sum_{i=1}^{n_j} (m_{i,j} - \mu_{meth,j})^2}
\end{align}

where $n_j$ is the number of methylation sites in sample $j$.

\subsection{Feature Engineering}

Our feature engineering approach extracts 98 comprehensive features across multiple categories, providing a holistic view of methylation patterns.

\subsubsection{Basic Methylation Features}

\textbf{Global Statistics:}

For each sample $j$, we compute fundamental methylation statistics:

\begin{align}
M_j &= \frac{1}{n_j} \sum_{i=1}^{n_j} m_{i,j} \quad \text{(Mean methylation)} \\
\text{Var}_j &= \frac{1}{n_j} \sum_{i=1}^{n_j} (m_{i,j} - M_j)^2 \quad \text{(Methylation variance)} \\
\text{Range}_j &= \max_i(m_{i,j}) - \min_i(m_{i,j}) \quad \text{(Methylation range)}
\end{align}

\textbf{Methylation Percentiles:}

The $q$-th percentile is defined as:
\begin{equation}
P_q = \inf\{x : F(x) \geq q/100\}
\end{equation}
where $F(x)$ is the empirical cumulative distribution function.

\textbf{Coverage-Weighted Statistics:}

To account for sequencing depth heterogeneity:
\begin{equation}
M_{w,j} = \frac{\sum_{i=1}^{n_j} (cov_{i,j} \times m_{i,j})}{\sum_{i=1}^{n_j} cov_{i,j}}
\end{equation}

\textbf{Methylation Categories:}

We define discrete methylation categories:
\begin{align}
H_j &= |\{i : m_{i,j} \geq 80\}| \quad \text{(High methylation sites)} \\
L_j &= |\{i : m_{i,j} \leq 20\}| \quad \text{(Low methylation sites)} \\
\text{Mod}_j &= |\{i : 20 < m_{i,j} < 80\}| \quad \text{(Moderate methylation sites)}
\end{align}

\subsubsection{Chromosomal Features}

For each chromosome $c$:

\textbf{Chromosome-specific Statistics:}
\begin{align}
M_{c,j} &= \frac{1}{n_{c,j}} \sum_{i \in c} m_{i,j} \quad \text{(Chromosome mean)} \\
R_{c,j} &= \frac{M_{c,j}}{M_j} \quad \text{(Relative chromosomal methylation)}
\end{align}

\textbf{Chromosomal Density:}
\begin{equation}
D_{c,j} = \frac{n_{c,j}}{(\text{end}_c - \text{start}_c)} \times 10^6 \quad \text{[sites per Mb]}
\end{equation}

\subsubsection{Regional Features (Window-based Analysis)}

\textbf{Sliding Window Analysis:}

For window size $w$, we define overlapping windows:
\begin{equation}
W_k = [k \times (w/2), k \times (w/2) + w]
\end{equation}

Window-specific statistics:
\begin{align}
M_{w,k} &= \frac{1}{|W_k|} \sum_{i \in W_k} m_i \quad \text{(Window methylation)} \\
\text{Var}_{w,k} &= \frac{1}{|W_k|} \sum_{i \in W_k} (m_i - M_{w,k})^2 \quad \text{(Window variance)}
\end{align}

\textbf{CpG Island-like Region Detection:}

We identify high-density regions using density threshold:
\begin{equation}
\rho_{threshold} = 10 \text{ sites/kb}
\end{equation}

For each potential island region $R$:
\begin{equation}
\rho_R = \frac{|R|}{(\text{length}_R / 1000)}
\end{equation}

If $\rho_R \geq \rho_{threshold}$, region $R$ is classified as high-density.

\textbf{Island Methylation Features:}
\begin{align}
M_{island} &= \frac{1}{|\text{Islands}|} \sum_{R \in \text{Islands}} M_R \\
\text{Var}_{island} &= \frac{1}{|\text{Islands}|} \sum_{R \in \text{Islands}} (M_R - M_{island})^2
\end{align}

\subsubsection{\MGMT{}-Specific Features}

\textbf{Canonical \MGMT{} Promoter:}

The canonical \MGMT{} promoter coordinates (GRCh38):
\begin{itemize}
    \item Chromosome: 10
    \item Start: 129,466,683
    \item End: 129,467,448
\end{itemize}

\textbf{\MGMT{} Promoter Statistics:}
\begin{align}
M_{\text{\MGMT}} &= \frac{1}{n_{\text{\MGMT}}} \sum_{i \in \text{\MGMT region}} m_i \\
H_{\text{\MGMT}} &= \frac{|\{i \in \text{\MGMT region} : m_i \geq 80\}|}{|\text{\MGMT region}|} \quad \text{(Hypermethylation ratio)}
\end{align}

\textbf{Extended \MGMT{} Region:}

Extended region: [129,464,683, 129,469,448] ($\pm$2kb around canonical promoter):
\begin{align}
M_{ext} &= \frac{1}{n_{ext}} \sum_{i \in \text{extended region}} m_i \\
H_{ext} &= \frac{|\{i \in \text{extended region} : m_i \geq 80\}|}{|\text{extended region}|}
\end{align}

\textbf{Chromosome 10 Context:}
\begin{equation}
R_{chr10} = \frac{M_{chr10}}{M_{global}}
\end{equation}

\subsubsection{Statistical Distribution Features}

\textbf{Distribution Shape Metrics:}

\textbf{Skewness \citep{joanes1998comparing}:}
\begin{equation}
\text{Skew} = \frac{\E[(X - \mu)^3]}{\sigma^3} = \frac{1}{n} \sum_{i=1}^{n} \left(\frac{x_i - \mu}{\sigma}\right)^3
\end{equation}

\textbf{Kurtosis \citep{joanes1998comparing}:}
\begin{equation}
\text{Kurt} = \frac{\E[(X - \mu)^4]}{\sigma^4} - 3 = \frac{1}{n} \sum_{i=1}^{n} \left(\frac{x_i - \mu}{\sigma}\right)^4 - 3
\end{equation}

\textbf{Entropy \citep{shannon1948mathematical}:}
\begin{equation}
H(X) = -\sum_{i=1}^{k} p_i \log_2(p_i)
\end{equation}
where $p_i$ is the probability of bin $i$ in the methylation histogram.

\textbf{Bimodality Assessment \citep{pfister2013good}:}

The bimodality coefficient:
\begin{equation}
BC = \frac{\text{Skew}^2 + 1}{\text{Kurt} + \frac{3(n-1)^2}{(n-2)(n-3)}}
\end{equation}

Values $> 0.555$ suggest bimodal distribution.

\textbf{Robust Statistics:}

\textbf{Median Absolute Deviation (MAD):}
\begin{equation}
\text{MAD} = \text{median}(|x_i - \text{median}(X)|)
\end{equation}

\textbf{Interquartile Range:}
\begin{equation}
\text{IQR} = Q_3 - Q_1
\end{equation}

\subsubsection{Spatial Correlation Features}

\textbf{Autocorrelation:}

For lag $\tau$:
\begin{equation}
r(\tau) = \frac{\sum_{t=1}^{n-\tau} (x_t - \mu)(x_{t+\tau} - \mu)}{\sum_{t=1}^{n} (x_t - \mu)^2}
\end{equation}

We compute autocorrelation at lags $\tau \in \{1, 5, 10\}$ for genomically ordered sites.

\textbf{Local Correlation:}
\begin{equation}
\rho_{local} = \frac{\sum_{i=1}^{n-1} (m_i - \mu)(m_{i+1} - \mu)}{\sqrt{\sum_{i=1}^{n} (m_i - \mu)^2 \times \sum_{i=1}^{n} (m_{i+1} - \mu)^2}}
\end{equation}

\subsection{Feature Scaling and Selection}

\subsubsection{Feature Scaling}

\textbf{Robust Scaling (Default):}
\begin{equation}
x_{scaled} = \frac{x - \text{median}(X)}{Q_3 - Q_1}
\end{equation}

\textbf{Standard Scaling:}
\begin{equation}
x_{scaled} = \frac{x - \mu}{\sigma}
\end{equation}

\subsubsection{Feature Selection}

\textbf{Recursive Feature Elimination (RFE):}

Using Random Forest as base estimator:
\begin{algorithm}
\caption{Recursive Feature Elimination}
\begin{algorithmic}
\STATE Initialize: $F = \{f_1, f_2, \ldots, f_p\}$ (all features)
\WHILE{$|F| > k$ (desired number of features)}
    \STATE Train model on features $F$
    \STATE Rank features by importance
    \STATE Remove least important feature from $F$
\ENDWHILE
\RETURN Selected features $F$
\end{algorithmic}
\end{algorithm}

\textbf{Feature Importance (Random Forest):}
\begin{equation}
I_j = \sum_{t \in T} \sum_{i \in N_t} p_i \times [\Delta_{impurity}(i, j) \times \mathbb{I}(v_i = j)]
\end{equation}

where $T$ is the set of trees, $N_t$ are nodes in tree $t$, $p_i$ is the proportion of samples reaching node $i$, and $\mathbb{I}(\cdot)$ is the indicator function.

\textbf{Univariate Feature Selection:}

Using F-statistic for classification:
\begin{equation}
F = \frac{\sum_{i=1}^{k} n_i(\mu_i - \mu)^2 / (k-1)}{\sum_{i=1}^{k} \sum_{j=1}^{n_i} (x_{ij} - \mu_i)^2 / (n-k)}
\end{equation}

where $k$ is the number of classes, $n_i$ is the number of samples in class $i$, and $\mu_i$ is the mean of class $i$.

\subsection{Machine Learning Models}

\subsubsection{Random Forest \citep{breiman2001random}}

\textbf{Ensemble Prediction:}
\begin{equation}
\hat{y} = \text{mode}\{T_1(x), T_2(x), \ldots, T_B(x)\}
\end{equation}

where $B$ is the number of trees and $T_i(x)$ is the prediction of the $i$-th tree.

\textbf{Bootstrap Sampling:} Each tree is trained on a bootstrap sample of size $n$ with replacement.

\textbf{Feature Bagging:} At each split, consider $\sqrt{p}$ random features where $p$ is the total number of features.

\textbf{Hyperparameters Optimized:}
\begin{itemize}
    \item $n_{estimators} \in \{100, 200, 300, 500\}$
    \item $max_{depth} \in \{\text{None}, 10, 20, 30\}$
    \item $min_{samples\_split} \in \{2, 5, 10\}$
    \item $min_{samples\_leaf} \in \{1, 2, 4\}$
\end{itemize}

\subsubsection{Gradient Boosting \citep{friedman2001greedy}}

\textbf{Additive Model:}
\begin{equation}
F_m(x) = F_{m-1}(x) + \gamma_m h_m(x)
\end{equation}

where $h_m(x)$ is the $m$-th weak learner.

\textbf{Loss Function (Log-loss):}
\begin{equation}
L(y, F(x)) = -[y \log(p) + (1-y) \log(1-p)]
\end{equation}

where $p = \frac{1}{1 + e^{-F(x)}}$.

\textbf{Gradient:}
\begin{equation}
r_{im} = -\left[\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\right]_{F=F_{m-1}}
\end{equation}

\subsubsection{Support Vector Machine \citep{cortes1995support}}

\textbf{Optimization Problem:}
\begin{align}
\min_{w,b,\xi} \quad & \frac{1}{2}\|w\|^2 + C \sum_{i=1}^{n} \xi_i \\
\text{subject to} \quad & y_i(w^T \phi(x_i) + b) \geq 1 - \xi_i \\
& \xi_i \geq 0
\end{align}

\textbf{RBF Kernel:}
\begin{equation}
K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)
\end{equation}

\subsubsection{Logistic Regression \citep{hosmer2013applied}}

\textbf{Logistic Function:}
\begin{equation}
p(y=1|x) = \frac{1}{1 + \exp(-(w^T x + b))}
\end{equation}

\textbf{Log-likelihood:}
\begin{equation}
\ell(w) = \sum_{i=1}^{n} [y_i \log(p_i) + (1-y_i) \log(1-p_i)]
\end{equation}

\textbf{Regularization:}
\begin{equation}
L_{total} = -\ell(w) + \lambda_1\|w\|_1 + \lambda_2\|w\|_2^2
\end{equation}

\subsubsection{Neural Network (MLP) \citep{rumelhart1986learning}}

\textbf{Forward Propagation:}
\begin{equation}
a^{(l+1)} = f(W^{(l)} a^{(l)} + b^{(l)})
\end{equation}

\textbf{Activation Functions:}
\begin{align}
\text{ReLU:} \quad & f(x) = \max(0, x) \\
\text{Tanh:} \quad & f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\end{align}

\textbf{Backpropagation:}
\begin{align}
\frac{\partial L}{\partial W^{(l)}} &= a^{(l-1)} (\delta^{(l)})^T \\
\frac{\partial L}{\partial b^{(l)}} &= \delta^{(l)}
\end{align}

\textbf{Adam Optimizer \citep{kingma2014adam}:}
\begin{align}
m_t &= \beta_1 m_{t-1} + (1-\beta_1)g_t \\
v_t &= \beta_2 v_{t-1} + (1-\beta_2)g_t^2 \\
\theta_{t+1} &= \theta_t - \alpha\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
\end{align}

where $\hat{m}_t = \frac{m_t}{1-\beta_1^t}$ and $\hat{v}_t = \frac{v_t}{1-\beta_2^t}$.

\subsubsection{Ensemble Methods}

\textbf{Soft Voting Classifier:}
\begin{equation}
\hat{y} = \argmax_c \sum_{i=1}^{m} w_i \times p_{i,c}
\end{equation}

where $m$ is the number of models, $w_i$ is the weight of model $i$, and $p_{i,c}$ is the predicted probability for class $c$ by model $i$.

\textbf{Model Weighting:}

Weights determined by cross-validation performance:
\begin{equation}
w_i = \frac{\text{AUC}_i}{\sum_{j=1}^{m} \text{AUC}_j}
\end{equation}

\subsection{Model Evaluation}

\subsubsection{Cross-Validation}

We employ stratified $k$-fold cross-validation to maintain class distribution in each fold.

\subsubsection{Performance Metrics}

\textbf{Classification Accuracy:}
\begin{equation}
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}

\textbf{Precision and Recall:}
\begin{align}
\text{Precision} &= \frac{TP}{TP + FP} \\
\text{Recall} &= \frac{TP}{TP + FN}
\end{align}

\textbf{F1-Score:}
\begin{equation}
F1 = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

\textbf{Specificity:}
\begin{equation}
\text{Specificity} = \frac{TN}{TN + FP}
\end{equation}

\textbf{Area Under ROC Curve (AUC-ROC):}

ROC curve points:
\begin{align}
TPR &= \frac{TP}{TP + FN} \quad \text{(True Positive Rate)} \\
FPR &= \frac{FP}{FP + TN} \quad \text{(False Positive Rate)}
\end{align}

AUC calculation using trapezoidal rule:
\begin{equation}
\text{AUC} = \sum_{i=1}^{n-1} (FPR_{i+1} - FPR_i) \times \frac{TPR_i + TPR_{i+1}}{2}
\end{equation}

\textbf{Precision-Recall AUC:}

Average Precision:
\begin{equation}
AP = \sum_{k=1}^{n} (R_k - R_{k-1}) \times P_k
\end{equation}

where $R_k$ and $P_k$ are recall and precision at threshold $k$.

\subsubsection{Model Calibration}

\textbf{Calibration Error \citep{niculescu2005predicting}:}
\begin{equation}
CE = \sum_{i=1}^{M} \frac{n_i}{n} |acc_i - conf_i|
\end{equation}

where $M$ is the number of bins, $n_i$ is the number of predictions in bin $i$, $acc_i$ is the accuracy in bin $i$, and $conf_i$ is the average confidence in bin $i$.

\textbf{Brier Score \citep{brier1950verification}:}
\begin{equation}
BS = \frac{1}{n} \sum_{i=1}^{n} (p_i - y_i)^2
\end{equation}

\subsubsection{Feature Importance Analysis}

\textbf{Permutation Importance:}
\begin{equation}
\text{Importance}_j = \text{Score}_{original} - \text{Score}_{permuted\_j}
\end{equation}

\textbf{SHAP Values \citep{lundberg2017unified}:}

Shapley value for feature $i$:
\begin{equation}
\phi_i = \sum_{S \subseteq F \setminus \{i\}} \frac{|S|!(|F|-|S|-1)!}{|F|!} [v(S \cup \{i\}) - v(S)]
\end{equation}

where $F$ is the set of all features, $S$ is a subset of features, and $v(S)$ is the model output for subset $S$.

\subsection{Statistical Analysis}

\subsubsection{Hypothesis Testing}

\textbf{Wilcoxon Rank-Sum Test (Mann-Whitney U):}

For comparing methylation distributions between groups:
\begin{equation}
U = R_1 - \frac{n_1(n_1+1)}{2}
\end{equation}

where $R_1$ is the sum of ranks for group 1.

\textbf{Kolmogorov-Smirnov Test:}

For testing distribution differences:
\begin{equation}
D = \max|F_1(x) - F_2(x)|
\end{equation}

\subsubsection{Multiple Testing Correction}

\textbf{Benjamini-Hochberg Procedure \citep{benjamini1995controlling}:}

\begin{algorithm}
\caption{Benjamini-Hochberg FDR Control}
\begin{algorithmic}
\STATE Sort p-values: $p_1 \leq p_2 \leq \ldots \leq p_m$
\STATE Find largest $k$ such that: $p_k \leq \frac{k}{m} \times \alpha$
\STATE Reject hypotheses $1, 2, \ldots, k$
\end{algorithmic}
\end{algorithm}

\textbf{False Discovery Rate:}
\begin{equation}
FDR = \E\left[\frac{V}{R} \mid R > 0\right] \times P(R > 0)
\end{equation}

where $V$ represents false discoveries and $R$ represents total rejections.

\subsection{Computational Implementation}

\subsubsection{Algorithmic Complexity}

\begin{itemize}
    \item \textbf{Data Loading:} $O(n)$ where $n$ = number of methylation sites
    \item \textbf{Feature Extraction:} $O(n \log n)$ for sorting + $O(n)$ for feature computation
    \item \textbf{Random Forest Training:} $O(B \times \sqrt{p} \times n \log n)$ where $B$ = trees, $p$ = features
    \item \textbf{SVM Training:} $O(n^3)$ for exact, $O(n^2)$ for SMO approximation
\end{itemize}

\subsubsection{Memory Optimization}

\textbf{Chunked Processing:}

For large datasets, process in chunks of size $C$:
\begin{equation}
\text{Memory usage} \approx C \times (\text{feature size} + \text{methylation data size})
\end{equation}

\textbf{Sparse Feature Representation:}

For binary features, use compressed sparse row (CSR) format:
\begin{equation}
\text{Memory reduction} = 1 - \frac{nnz + n + 1}{n \times p}
\end{equation}

where $nnz$ is the number of non-zero elements.

\subsection{Validation Framework}

\subsubsection{Internal Validation}

\textbf{Nested Cross-Validation:}
\begin{itemize}
    \item Outer loop: Performance estimation
    \item Inner loop: Hyperparameter optimization
\end{itemize}

\subsubsection{External Validation}

\textbf{Independent Cohort Testing:} Model trained on cohort A, tested on cohort B.

\textbf{Cross-Platform Validation:} Validation across different sequencing platforms.

\section{Software Implementation}

\subsection{Core Dependencies}

The pipeline requires the following software dependencies:

\begin{itemize}
    \item Python ($\geq$ 3.9)
    \item pandas ($\geq$ 1.5.0): Data manipulation
    \item numpy ($\geq$ 1.21.0): Numerical computing
    \item scikit-learn ($\geq$ 1.2.0): Machine learning algorithms \citep{pedregosa2011scikit}
    \item scipy ($\geq$ 1.8.0): Statistical functions
    \item matplotlib ($\geq$ 3.5.0): Plotting
    \item seaborn ($\geq$ 0.11.0): Statistical visualization
    \item Nextflow ($\geq$ 22.0.0): Pipeline orchestration \citep{di2017nextflow}
\end{itemize}

\subsection{Performance Benchmarks}

\textbf{Single Sample Processing Times:}
\begin{itemize}
    \item Data loading: $\sim$30 seconds (1M sites)
    \item Feature extraction: $\sim$2 minutes
    \item Model training: $\sim$5--10 minutes
    \item Prediction: $\sim$1 second
\end{itemize}

\textbf{Memory Requirements:}
\begin{itemize}
    \item Base: $\sim$2GB per sample
    \item Peak: $\sim$8GB during feature extraction
    \item Scalable: Linear with sample count
\end{itemize}

\section{Quality Control and Limitations}

\subsection{Technical Limitations}

\begin{enumerate}
    \item \textbf{Coverage Dependency:} Low-coverage regions may introduce bias
    \item \textbf{Platform Specificity:} Optimized for Oxford Nanopore data
    \item \textbf{Reference Genome:} Assumes GRCh38 human reference
\end{enumerate}

\subsection{Biological Assumptions}

\begin{enumerate}
    \item \textbf{Methylation Stability:} Assumes stable methylation patterns within samples
    \item \textbf{CpG Context:} Focuses on CpG methylation (5mC)
    \item \textbf{Tissue Specificity:} May require tissue-specific calibration
\end{enumerate}

\subsection{Statistical Assumptions}

\begin{enumerate}
    \item \textbf{Independence:} Assumes methylation sites are conditionally independent given covariates
    \item \textbf{Stationarity:} Assumes methylation patterns are stationary across samples
    \item \textbf{Class Balance:} Performance may degrade with extreme class imbalance
\end{enumerate}

\section{Reproducibility}

\subsection{Random Seed Control}

All stochastic processes employ controlled random seeds:
\begin{lstlisting}[language=Python]
import numpy as np
from sklearn.utils import check_random_state

np.random.seed(42)
random_state = check_random_state(42)
\end{lstlisting}

\subsection{Version Control}

Software versions are controlled via:
\begin{itemize}
    \item \texttt{requirements.txt}: Python package versions
    \item \texttt{environment.yml}: Conda environment specification
    \item Docker containers: Complete computational environment
\end{itemize}

\subsection{Parameter Logging}

All hyperparameters and configuration settings are automatically logged in JSON format for complete reproducibility.

\section{Conclusion}

We have presented a comprehensive computational framework for predicting \MGMT{} methylation status from genome-wide methylation data. The pipeline incorporates advanced feature engineering, multiple machine learning algorithms, and rigorous evaluation metrics. The modular design enables flexible deployment across various computational environments, from local workstations to high-performance computing clusters.

The approach extends traditional \MGMT{} analysis by incorporating genome-wide methylation context, potentially improving prediction accuracy and providing insights into broader epigenetic patterns associated with treatment response. The implementation follows best practices for reproducible computational biology, with comprehensive documentation, version control, and standardized output formats.

Future enhancements may include integration with additional omics data types, development of transfer learning approaches for cross-cohort applications, and implementation of uncertainty quantification methods for clinical decision support.

\section*{Data Availability}

The computational pipeline and associated documentation are available at: \url{https://github.com/your-repository/mgmt-pipeline}. Example datasets and validation cohorts should be obtained through appropriate institutional and ethical approvals.

\section*{Competing Interests}

The authors declare no competing interests related to the computational methods described in this manuscript.

\section*{Acknowledgments}

We acknowledge the Oxford Nanopore Technologies community for the development of modkit and the open-source bioinformatics community for the foundational tools utilized in this pipeline.

\bibliographystyle{nature}
\bibliography{references}

\end{document}
