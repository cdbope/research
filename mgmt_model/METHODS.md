# Methods: MGMT Methylation Detection Pipeline

## Abstract

This document describes the computational methods implemented in the MGMT methylation detection pipeline for predicting MGMT promoter methylation status from genome-wide DNA methylation data obtained through Oxford Nanopore Technologies sequencing and processed with modkit. The pipeline employs comprehensive feature engineering, multiple machine learning algorithms, and ensemble methods to achieve robust prediction performance.

## 1. Introduction

MGMT (O6-methylguanine-DNA methyltransferase) promoter methylation is a critical biomarker for predicting treatment response to alkylating agents in glioblastoma patients [1,2]. Traditional analysis focuses solely on the MGMT promoter region (chr10:129,466,683-129,467,448, GRCh38), but emerging evidence suggests that genome-wide methylation patterns may provide additional predictive information [3,4].

## 2. Data Input and Preprocessing

### 2.1 Bedmethyl File Format

The pipeline processes bedmethyl files generated by Oxford Nanopore's modkit package [5]. Each record contains:

```
chromosome  start  end  mod_type  score  strand  thick_start  thick_end  rgb  coverage  percent_modified  n_modified  n_canonical  n_other_mod  n_delete  n_fail  n_diff
```

Where:
- `percent_modified`: Methylation percentage (0-100%)
- `coverage`: Number of reads covering the position
- `n_modified`: Number of modified calls
- `n_canonical`: Number of canonical (unmethylated) calls

### 2.2 Quality Filtering

Methylation sites are filtered based on coverage threshold:

```
S_filtered = {s ∈ S | coverage(s) ≥ θ_cov}
```

Where:
- S = set of all methylation sites
- θ_cov = minimum coverage threshold (default: 3)

### 2.3 Data Validation

For each sample, we compute quality metrics:

**Coverage Statistics:**
```
μ_cov = (1/n) Σ(i=1 to n) coverage_i
σ_cov = √[(1/n) Σ(i=1 to n) (coverage_i - μ_cov)²]
```

**Methylation Statistics:**
```
μ_meth = (1/n) Σ(i=1 to n) percent_modified_i
σ_meth = √[(1/n) Σ(i=1 to n) (percent_modified_i - μ_meth)²]
```

## 3. Feature Engineering

### 3.1 Basic Methylation Features

#### 3.1.1 Global Statistics

For each sample j, we compute:

**Mean Methylation:**
```
M_j = (1/n_j) Σ(i=1 to n_j) m_i
```

**Methylation Variance:**
```
Var_j = (1/n_j) Σ(i=1 to n_j) (m_i - M_j)²
```

**Methylation Range:**
```
Range_j = max(m_i) - min(m_i)
```

**Methylation Percentiles:**
```
P_q = inf{x : F(x) ≥ q/100}
```
Where F(x) is the empirical cumulative distribution function.

#### 3.1.2 Coverage-Weighted Statistics

**Weighted Mean Methylation:**
```
M_w = Σ(i=1 to n) (c_i × m_i) / Σ(i=1 to n) c_i
```

Where c_i is the coverage at position i.

#### 3.1.3 Methylation Categories

**High Methylation Sites:**
```
H = |{i : m_i ≥ 80}|
```

**Low Methylation Sites:**
```
L = |{i : m_i ≤ 20}|
```

**Moderate Methylation Sites:**
```
Mod = |{i : 20 < m_i < 80}|
```

### 3.2 Chromosomal Features

For each chromosome c:

**Chromosome-specific Mean:**
```
M_c = (1/n_c) Σ(i∈c) m_i
```

**Relative Chromosomal Methylation:**
```
R_c = M_c / M_global
```

**Chromosomal Density:**
```
D_c = n_c / (end_c - start_c) × 10^6  [sites per Mb]
```

### 3.3 Regional Features (Window-based Analysis)

#### 3.3.1 Sliding Window Analysis

For window size w, we define windows:
```
W_k = [k × (w/2), k × (w/2) + w]
```

**Window Methylation:**
```
M_w(k) = (1/|W_k|) Σ(i∈W_k) m_i
```

**Window Variance:**
```
Var_w(k) = (1/|W_k|) Σ(i∈W_k) (m_i - M_w(k))²
```

#### 3.3.2 CpG Island-like Region Detection

CpG density threshold:
```
ρ_threshold = 10 sites/kb
```

For each potential island region R:
```
ρ_R = |R| / (length_R / 1000)
```

If ρ_R ≥ ρ_threshold, R is classified as high-density region.

**Island Methylation Features:**
```
M_island = (1/|Islands|) Σ(R∈Islands) M_R
Var_island = (1/|Islands|) Σ(R∈Islands) (M_R - M_island)²
```

### 3.4 MGMT-Specific Features

#### 3.4.1 Canonical MGMT Promoter

MGMT promoter coordinates (GRCh38):
- Chromosome: 10
- Start: 129,466,683
- End: 129,467,448

**MGMT Promoter Methylation:**
```
M_MGMT = (1/n_MGMT) Σ(i∈MGMT_region) m_i
```

**MGMT Hypermethylation Ratio:**
```
H_MGMT = |{i ∈ MGMT_region : m_i ≥ 80}| / |MGMT_region|
```

#### 3.4.2 Extended MGMT Region

Extended region: [129,464,683, 129,469,448] (±2kb)

**Extended MGMT Features:**
```
M_ext = (1/n_ext) Σ(i∈extended_region) m_i
H_ext = |{i ∈ extended_region : m_i ≥ 80}| / |extended_region|
```

#### 3.4.3 Chromosome 10 Context

**Chr10 vs Global Ratio:**
```
R_chr10 = M_chr10 / M_global
```

### 3.5 Statistical Distribution Features

#### 3.5.1 Distribution Shape

**Skewness [6]:**
```
Skew = E[(X - μ)³] / σ³ = (1/n) Σ(i=1 to n) ((x_i - μ)/σ)³
```

**Kurtosis [6]:**
```
Kurt = E[(X - μ)⁴] / σ⁴ - 3 = (1/n) Σ(i=1 to n) ((x_i - μ)/σ)⁴ - 3
```

**Entropy [7]:**
```
H(X) = -Σ(i=1 to k) p_i log₂(p_i)
```
Where p_i is the probability of bin i in the methylation histogram.

#### 3.5.2 Bimodality Assessment

**Bimodality Coefficient [8]:**
```
BC = (Skew² + 1) / (Kurt + 3(n-1)²/((n-2)(n-3)))
```

Values > 0.555 suggest bimodal distribution.

#### 3.5.3 Robust Statistics

**Median Absolute Deviation (MAD):**
```
MAD = median(|x_i - median(X)|)
```

**Interquartile Range:**
```
IQR = Q₃ - Q₁
```

### 3.6 Spatial Correlation Features

#### 3.6.1 Autocorrelation

For lag τ:
```
r(τ) = Σ(t=1 to n-τ) (x_t - μ)(x_{t+τ} - μ) / Σ(t=1 to n) (x_t - μ)²
```

We compute autocorrelation at lags τ ∈ {1, 5, 10} for genomically ordered sites.

#### 3.6.2 Local Correlation

**Neighboring Site Correlation:**
```
ρ_local = Σ(i=1 to n-1) (m_i - μ)(m_{i+1} - μ) / √[Σ(i=1 to n) (m_i - μ)² × Σ(i=1 to n) (m_{i+1} - μ)²]
```

## 4. Feature Scaling and Selection

### 4.1 Feature Scaling

#### 4.1.1 Robust Scaling (Default)

```
x_scaled = (x - median(X)) / (Q₃ - Q₁)
```

Where Q₁ and Q₃ are the first and third quartiles.

#### 4.1.2 Standard Scaling

```
x_scaled = (x - μ) / σ
```

### 4.2 Feature Selection

#### 4.2.1 Recursive Feature Elimination (RFE)

Using Random Forest as base estimator:

1. Train model on all features
2. Rank features by importance
3. Remove least important feature
4. Repeat until desired number of features

**Feature Importance (Random Forest):**
```
I_j = Σ(t∈T) Σ(i∈N_t) p_i × [Δ_impurity(i, j) × I(v_i = j)]
```

Where:
- T = set of trees
- N_t = nodes in tree t
- p_i = proportion of samples reaching node i
- Δ_impurity = impurity decrease from split
- I(v_i = j) = indicator function for feature j at node i

#### 4.2.2 Univariate Feature Selection

Using F-statistic for classification:
```
F = Σ(i=1 to k) n_i(μ_i - μ)² / (k-1) / Σ(i=1 to k) Σ(j=1 to n_i) (x_{ij} - μ_i)² / (n-k)
```

Where:
- k = number of classes
- n_i = number of samples in class i
- μ_i = mean of class i

## 5. Machine Learning Models

### 5.1 Random Forest [9]

**Ensemble Prediction:**
```
ŷ = mode{T₁(x), T₂(x), ..., T_B(x)}
```

**Bootstrap Sampling:**
Each tree trained on bootstrap sample of size n with replacement.

**Feature Bagging:**
At each split, consider √p random features (p = total features).

**Hyperparameters Optimized:**
- n_estimators ∈ {100, 200, 300, 500}
- max_depth ∈ {None, 10, 20, 30}
- min_samples_split ∈ {2, 5, 10}
- min_samples_leaf ∈ {1, 2, 4}

### 5.2 Gradient Boosting [10]

**Additive Model:**
```
F_m(x) = F_{m-1}(x) + γ_m h_m(x)
```

Where h_m(x) is the m-th weak learner.

**Loss Function (Log-loss):**
```
L(y, F(x)) = -[y log(p) + (1-y) log(1-p)]
```

**Gradient:**
```
r_{im} = -[∂L(y_i, F(x_i))/∂F(x_i)]_{F=F_{m-1}}
```

**Hyperparameters Optimized:**
- n_estimators ∈ {100, 200, 300}
- learning_rate ∈ {0.01, 0.1, 0.2}
- max_depth ∈ {3, 5, 7, 10}

### 5.3 Support Vector Machine [11]

**Optimization Problem:**
```
min_{w,b,ξ} (1/2)||w||² + C Σ(i=1 to n) ξ_i
```

Subject to:
```
y_i(w^T φ(x_i) + b) ≥ 1 - ξ_i
ξ_i ≥ 0
```

**RBF Kernel:**
```
K(x_i, x_j) = exp(-γ ||x_i - x_j||²)
```

**Hyperparameters Optimized:**
- C ∈ {0.1, 1, 10, 100}
- γ ∈ {'scale', 'auto', 0.001, 0.01, 0.1, 1}

### 5.4 Logistic Regression [12]

**Logistic Function:**
```
p(y=1|x) = 1 / (1 + exp(-(w^T x + b)))
```

**Log-likelihood:**
```
ℓ(w) = Σ(i=1 to n) [y_i log(p_i) + (1-y_i) log(1-p_i)]
```

**Regularization:**
```
L_total = -ℓ(w) + λ₁||w||₁ + λ₂||w||₂²
```

**Hyperparameters Optimized:**
- C ∈ {0.001, 0.01, 0.1, 1, 10, 100}
- penalty ∈ {'l1', 'l2', 'elasticnet'}

### 5.5 Neural Network (MLP) [13]

**Forward Propagation:**
```
a^{(l+1)} = f(W^{(l)} a^{(l)} + b^{(l)})
```

**Activation Functions:**
- ReLU: f(x) = max(0, x)
- Tanh: f(x) = (e^x - e^{-x})/(e^x + e^{-x})

**Backpropagation:**
```
∂L/∂W^{(l)} = a^{(l-1)} (δ^{(l)})^T
∂L/∂b^{(l)} = δ^{(l)}
```

**Adam Optimizer [14]:**
```
m_t = β₁m_{t-1} + (1-β₁)g_t
v_t = β₂v_{t-1} + (1-β₂)g_t²
θ_{t+1} = θ_t - α(m̂_t/(√v̂_t + ε))
```

### 5.6 Ensemble Methods

#### 5.6.1 Voting Classifier

**Soft Voting:**
```
ŷ = argmax_c Σ(i=1 to m) w_i × p_{i,c}
```

Where:
- m = number of models
- w_i = weight of model i
- p_{i,c} = predicted probability for class c by model i

#### 5.6.2 Model Weighting

Weights determined by cross-validation performance:
```
w_i = AUC_i / Σ(j=1 to m) AUC_j
```

## 6. Model Evaluation

### 6.1 Cross-Validation

**Stratified K-Fold CV:**
Maintains class distribution in each fold.

**Performance Metrics:**

#### 6.1.1 Classification Accuracy
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
```

#### 6.1.2 Precision and Recall
```
Precision = TP / (TP + FP)
Recall = TP / (TP + FN)
```

#### 6.1.3 F1-Score
```
F1 = 2 × (Precision × Recall) / (Precision + Recall)
```

#### 6.1.4 Specificity
```
Specificity = TN / (TN + FP)
```

#### 6.1.5 Area Under ROC Curve (AUC-ROC)

**ROC Curve Points:**
```
TPR = TP / (TP + FN)  [True Positive Rate]
FPR = FP / (FP + TN)  [False Positive Rate]
```

**AUC Calculation (Trapezoidal Rule):**
```
AUC = Σ(i=1 to n-1) (FPR_{i+1} - FPR_i) × (TPR_i + TPR_{i+1})/2
```

#### 6.1.6 Precision-Recall AUC

**Average Precision:**
```
AP = Σ(k=1 to n) (R_k - R_{k-1}) × P_k
```

Where R_k and P_k are recall and precision at threshold k.

### 6.2 Model Calibration

**Calibration Error [15]:**
```
CE = Σ(i=1 to M) (n_i/n) |acc_i - conf_i|
```

Where:
- M = number of bins
- n_i = number of predictions in bin i
- acc_i = accuracy in bin i
- conf_i = average confidence in bin i

**Brier Score [16]:**
```
BS = (1/n) Σ(i=1 to n) (p_i - y_i)²
```

### 6.3 Feature Importance Analysis

#### 6.3.1 Permutation Importance

```
Importance_j = Score_original - Score_permuted_j
```

#### 6.3.2 SHAP Values [17]

**Shapley Value:**
```
φ_i = Σ_{S⊆F\{i}} (|S|!(|F|-|S|-1)!)/(|F|!) [v(S∪{i}) - v(S)]
```

Where:
- F = set of all features
- S = subset of features
- v(S) = model output for subset S

## 7. Statistical Analysis

### 7.1 Hypothesis Testing

**Wilcoxon Rank-Sum Test (Mann-Whitney U):**
For comparing methylation distributions between groups:

```
U = R₁ - n₁(n₁+1)/2
```

Where R₁ is the sum of ranks for group 1.

**Kolmogorov-Smirnov Test:**
For testing distribution differences:

```
D = max|F₁(x) - F₂(x)|
```

### 7.2 Multiple Testing Correction

**Benjamini-Hochberg Procedure [18]:**

1. Sort p-values: p₁ ≤ p₂ ≤ ... ≤ pₘ
2. Find largest k such that: p_k ≤ (k/m) × α
3. Reject hypotheses 1, 2, ..., k

**False Discovery Rate:**
```
FDR = E[V/R | R > 0] × P(R > 0)
```

Where V = false discoveries, R = total rejections.

## 8. Computational Implementation

### 8.1 Algorithmic Complexity

**Data Loading:** O(n) where n = number of methylation sites
**Feature Extraction:** O(n log n) for sorting + O(n) for feature computation
**Model Training:** 
- Random Forest: O(B × √p × n log n) where B = trees, p = features
- SVM: O(n³) for exact, O(n²) for SMO approximation

### 8.2 Memory Optimization

**Chunked Processing:**
For large datasets, process in chunks of size C:

```
Memory_usage ≈ C × (feature_size + methylation_data_size)
```

**Sparse Feature Representation:**
For binary features, use compressed sparse row (CSR) format:

```
Memory_reduction = 1 - (nnz + n + 1)/(n × p)
```

Where nnz = number of non-zero elements.

## 9. Validation Framework

### 9.1 Internal Validation

**Time Series Split:**
For temporal datasets:
- Training: samples 1 to t
- Testing: samples t+1 to t+k

**Nested Cross-Validation:**
- Outer loop: Performance estimation
- Inner loop: Hyperparameter optimization

### 9.2 External Validation

**Independent Cohort Testing:**
Model trained on cohort A, tested on cohort B.

**Cross-Platform Validation:**
Validation across different sequencing platforms (ONT vs. Illumina).

## 10. Quality Control Metrics

### 10.1 Data Quality

**Coverage Adequacy:**
```
Q_coverage = |{sites : coverage ≥ θ}| / |total_sites|
```

**Methylation Call Quality:**
```
Q_calls = Σ(modified_calls + canonical_calls) / Σ(total_calls)
```

### 10.2 Model Quality

**Cross-Validation Stability:**
```
Stability = 1 - σ_CV / μ_CV
```

Where σ_CV and μ_CV are standard deviation and mean of CV scores.

**Prediction Confidence:**
```
Confidence = max(P(class_1), P(class_2))
```

## 11. Limitations and Assumptions

### 11.1 Technical Limitations

1. **Coverage Dependency:** Low-coverage regions may introduce bias
2. **Platform Specificity:** Optimized for Oxford Nanopore data
3. **Reference Genome:** Assumes GRCh38 human reference

### 11.2 Biological Assumptions

1. **Methylation Stability:** Assumes stable methylation patterns
2. **CpG Context:** Focuses on CpG methylation (5mC)
3. **Tissue Specificity:** May require tissue-specific calibration

### 11.3 Statistical Assumptions

1. **Independence:** Assumes methylation sites are conditionally independent
2. **Stationarity:** Assumes methylation patterns are stationary across samples
3. **Class Balance:** Performance may degrade with extreme class imbalance

## 12. Software Implementation

### 12.1 Dependencies

**Core Libraries:**
- pandas (≥1.5.0): Data manipulation
- numpy (≥1.21.0): Numerical computing
- scikit-learn (≥1.2.0): Machine learning algorithms
- scipy (≥1.8.0): Statistical functions

**Visualization:**
- matplotlib (≥3.5.0): Plotting
- seaborn (≥0.11.0): Statistical visualization

**Workflow Management:**
- Nextflow (≥22.0.0): Pipeline orchestration

### 12.2 Performance Benchmarks

**Single Sample Processing:**
- Data loading: ~30 seconds (1M sites)
- Feature extraction: ~2 minutes
- Model training: ~5-10 minutes
- Prediction: ~1 second

**Memory Requirements:**
- Base: ~2GB per sample
- Peak: ~8GB during feature extraction
- Scalable: Linear with sample count

## 13. Reproducibility

### 13.1 Random Seed Control

All stochastic processes use controlled random seeds:
```python
np.random.seed(42)
sklearn.utils.check_random_state(42)
```

### 13.2 Version Control

Software versions are logged and controlled via:
- requirements.txt (Python packages)
- environment.yml (Conda environment)
- Docker containers (complete environment)

### 13.3 Parameter Logging

All hyperparameters and configuration settings are automatically logged in JSON format for complete reproducibility.

## References

[1] Hegi, M.E., et al. (2005). MGMT gene silencing and benefit from temozolomide in glioblastoma. New England Journal of Medicine, 352(10), 997-1003.

[2] Stupp, R., et al. (2009). Effects of radiotherapy with concomitant and adjuvant temozolomide versus radiotherapy alone on survival in glioblastoma in a randomised phase III study: 5-year analysis of the EORTC-NCIC trial. The Lancet Oncology, 10(5), 459-466.

[3] Bady, P., et al. (2012). MGMT methylation analysis of glioblastoma on the Infinium methylation BeadChip identifies two distinct CpG regions associated with gene silencing and outcome, yielding a prediction model for comparisons across datasets, tumor grades, and CIMP-status. Acta Neuropathologica, 124(4), 547-560.

[4] Capper, D., et al. (2018). DNA methylation-based classification of central nervous system tumours. Nature, 555(7697), 469-474.

[5] Oxford Nanopore Technologies. (2023). modkit: A bioinformatics tool for working with modified bases. https://github.com/nanoporetech/modkit

[6] Joanes, D.N., & Gill, C.A. (1998). Comparing measures of sample skewness and kurtosis. Journal of the Royal Statistical Society: Series D (The Statistician), 47(1), 183-189.

[7] Shannon, C.E. (1948). A mathematical theory of communication. The Bell System Technical Journal, 27(3), 379-423.

[8] Pfister, R., et al. (2013). Good things peak in pairs: a note on the bimodality coefficient. Frontiers in Psychology, 4, 700.

[9] Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5-32.

[10] Friedman, J.H. (2001). Greedy function approximation: a gradient boosting machine. Annals of Statistics, 29(5), 1189-1232.

[11] Cortes, C., & Vapnik, V. (1995). Support-vector networks. Machine Learning, 20(3), 273-297.

[12] Hosmer Jr, D.W., et al. (2013). Applied logistic regression (Vol. 398). John Wiley & Sons.

[13] Rumelhart, D.E., et al. (1986). Learning representations by back-propagating errors. Nature, 323(6088), 533-536.

[14] Kingma, D.P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.

[15] Niculescu-Mizil, A., & Caruana, R. (2005). Predicting good probabilities with supervised learning. Proceedings of the 22nd International Conference on Machine Learning, 625-632.

[16] Brier, G.W. (1950). Verification of forecasts expressed in terms of probability. Monthly Weather Review, 78(1), 1-3.

[17] Lundberg, S.M., & Lee, S.I. (2017). A unified approach to interpreting model predictions. Advances in Neural Information Processing Systems, 30, 4765-4774.

[18] Benjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society: Series B (Methodological), 57(1), 289-300.

[19] Pedregosa, F., et al. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2825-2830.

[20] Di Tommaso, P., et al. (2017). Nextflow enables reproducible computational workflows. Nature Biotechnology, 35(4), 316-319.

---

**Corresponding Author Information:**
For technical questions regarding the implementation of these methods, please refer to the pipeline documentation and source code available in the repository.

**Data Availability:**
The pipeline is designed to work with publicly available bedmethyl files generated by Oxford Nanopore's modkit package. Example datasets and validation cohorts should be obtained through appropriate institutional and ethical approvals.

**Conflict of Interest:**
The authors declare no competing interests related to the computational methods described in this document.
